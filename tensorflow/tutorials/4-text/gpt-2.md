

# GPT-2

Paper: [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)

GitHub repo: https://github.com/openai/gpt-2

> * This repository is meant to be a starting point for researchers and engineers to experiment with GPT-2.
> * For basic information, see our [model card](https://github.com/openai/gpt-2/blob/master/model_card.md).

Blog posts

* Original blog post: [Better Language Models and Their Implications](https://blog.openai.com/better-language-models/)
* 6 month follow-up post: [GPT-2: 6-Month Follow-Up](https://openai.com/blog/gpt-2-6-month-follow-up/)
* Final post: [GPT-2: 1.5B Release](https://www.openai.com/blog/gpt-2-1-5b-release/)

Dataset release: [gpt-2-output-dataset](https://github.com/openai/gpt-2-output-dataset)





