# -*- coding: utf-8 -*-
"""1-word_embeddings-basic_test.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1H6yr2J5-Zgq-G5fMhgcaQyNt6uQwGITR
##### Copyright 2019 The TensorFlow Authors.
"""
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
# Word embeddings

* The explanations are manually removed by T Kim.
  * For the theory, refer to the manual at
    https://www.tensorflow.org/text/guide/word_embeddings
"""

# Vocabulary size and number of words in a sequence.
vocab_size = 10000
sequence_length = 100

## Setup
import os
import shutil
import tensorflow as tf

## Download the IMDb Dataset
url = "https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
dataset = tf.keras.utils.get_file("aclImdb_v1.tar.gz", url,
                                  untar=True, cache_dir='.',
                                  cache_subdir='')
dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')
os.listdir(dataset_dir)

train_dir = os.path.join(dataset_dir, 'train')
os.listdir(train_dir)

# The `train` directory has additional folders
remove_dir = os.path.join(train_dir, 'unsup')
shutil.rmtree(remove_dir)

# Create a `tf.data.Dataset`
batch_size = 1024
seed = 123
train_ds = tf.keras.preprocessing.text_dataset_from_directory(
    'aclImdb/train', batch_size=batch_size, validation_split=0.2,
    subset='training', seed=seed)
val_ds = tf.keras.preprocessing.text_dataset_from_directory(
    'aclImdb/train', batch_size=batch_size, validation_split=0.2,
    subset='validation', seed=seed)

for text_batch, label_batch in train_ds.take(1):
  for i in range(5):
    print(label_batch[i].numpy(), text_batch.numpy()[i])

## Using the Embedding layer
# [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)

# Embed a 1,000 word vocabulary into 5 dimensions.
#   the weights for the embedding are randomly initialized
embedding_layer = tf.keras.layers.Embedding(1000, 5)
result = embedding_layer(tf.constant([1, 2, 3]))  # Test
result.numpy()                                    # Test

# The Embedding layer 
#   Pass it a (2, 3) input batch and the output is (2, 3, N).
# Input: (samples, sequence_length)
#   a 2D tensor of integers , e.g. 
#     (32, 10) (batch of 32 sequences of length 10) or
#     (64, 15) (batch of 64 sequences of length 15).
# Output: (samples, sequence_length, embedding_dimensionality)
#   a 3D floating point tensor, e.g. 
#     (32, 10, 5)
#     (64, 15, 5)
result = embedding_layer(tf.constant([[0, 1, 2], [3, 4, 5]]))  # Test
result.shape                                                   # Test