# -*- coding: utf-8 -*-
"""1-word_embeddings.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1H6yr2J5-Zgq-G5fMhgcaQyNt6uQwGITR
##### Copyright 2019 The TensorFlow Authors.
"""
#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
# Word embeddings

The most explanations are removed by T Kim.
* For details, refer to the manual 
    https://www.tensorflow.org/text/guide/word_embeddings

1_word_embeddings-basic_test.py
  refer to this basic test source code     
    to learn more about the data download and embedding layer.
"""

# Create a `tf.data.Dataset`
batch_size = 1024
seed = 123
validation_split = 0.2
epochs = 15

# Vocabulary size and number of words in a sequence.
vocab_size = 10000
sequence_length = 100
embedding_dim = 16

## Setup
import io
import os
import re
import shutil
import string
import tensorflow as tf

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization

## Download the IMDb Dataset
url = "https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
dataset = tf.keras.utils.get_file("aclImdb_v1.tar.gz", url,
                                  untar=True, cache_dir='.',
                                  cache_subdir='')
dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')
train_dir = os.path.join(dataset_dir, 'train')
# The `train` directory has additional folders
remove_dir = os.path.join(train_dir, 'unsup')
shutil.rmtree(remove_dir)

train_ds = tf.keras.preprocessing.text_dataset_from_directory(
    'aclImdb/train', batch_size=batch_size, validation_split=validation_split,
    subset='training', seed=seed)
val_ds = tf.keras.preprocessing.text_dataset_from_directory(
    'aclImdb/train', batch_size=batch_size, validation_split=validation_split,
    subset='validation', seed=seed)

## Configure the dataset for performance
#    Ensure the dataset I/O doesn't become a bottleneck.
#    .cache() keeps data in memory after it's loaded off disk.
#    .prefetch() overlaps data preprocessing & model execution while training. 
#  [data performance guide](https://www.tensorflow.org/guide/data_performance)

AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

## Text preprocessing
# For, TextVectorization, refer to:
#  [Text Classification](https://www.tensorflow.org/tutorials/keras/text_classification)

def custom_standardization(input_data):
  # Strip HTML break tags '<br />'
  lowercase = tf.strings.lower(input_data)
  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')
  return tf.strings.regex_replace(stripped_html,
                                  '[%s]' % re.escape(string.punctuation), '')

"""## Compile and train the model

You will use [TensorBoard](https://www.tensorflow.org/tensorboard) to visualize metrics including loss and accuracy. Create a `tf.keras.callbacks.TensorBoard`.
"""
# TextVectorization is to normalize, split, and map strings to integers.
vectorize_layer = TextVectorization(
    standardize=custom_standardization,
    max_tokens=vocab_size,
    output_mode='int',
    output_sequence_length=sequence_length)

# Make a text-only dataset without labels; call adapt to build the vocabulary.
text_ds = train_ds.map(lambda x, y: x)
vectorize_layer.adapt(text_ds)

## Create a classification model
#    This is a CBOW (Continuous Bag Of Words) style model.
# Caution: This model doesn't use masking, so the zero-padding is used 
#   as part of the input and hence the padding length may affect the output.
#   To fix this, refer to [masking and padding guide](https://www.tensorflow.org/guide/keras/masking_and_padding).

# vectorize_layer is the 1st layer of your end-to-end classification model.
# Embedding
#    (batch, sequence) -> embedding_layer -> (batch, sequence, embedding)
#             (32, 10) ->                 -> (32, 10, 5)
#             (64, 15) ->                 -> (64, 15, 5)
# GlobalAveragePooling1D
#   returns a fixed-length output vector for each example 
#   by averaging over the sequence dimension.
#   Pooling converts a variable-length vector to a fixed representation
#     before passing it to a Dense layer.
#   standard approaches: pooling layer, RNN, Attention
#   For the next step, you may see
#     [Text Classification with an RNN](https://www.tensorflow.org/text/tutorials/text_classification_rnn)

model = Sequential([
  vectorize_layer,
  Embedding(vocab_size, embedding_dim, name="embedding"),
  GlobalAveragePooling1D(),
  Dense(16, activation='relu'),
  Dense(1)
])
model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])

tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir="logs")
model.fit(train_ds, validation_data=val_ds,
          epochs=epochs, callbacks=[tensorboard_callback])

model.summary()
# With this approach the model reaches a validation accuracy of around 84%.
# Note: the model is overfitting since training accuracy is higher

## Visualize the model metrics in TensorBoard.
#    #1. Run tensorboard in a terminal
#      $ tensorboard --logdir logs
#        ...
#      Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all
#      TensorBoard 2.5.0 at http://localhost:6006/ (Press CTRL+C to quit)
#    #2. Open a web browser
#    #3. Go to http://localhost:6006/

## Retrieve the trained word embeddings and save them to disk
#  get_weights() retrieves weights of the Embedding layer in the model. 
#    The weights matrix is of shape (vocab_size, embedding_dimension).
#  get_vocabulary() provides the vocabulary to build a metadata file
#    with one token per line.

weights = model.get_layer('embedding').get_weights()[0]
vocab = vectorize_layer.get_vocabulary()

out_v = io.open('vectors.tsv', 'w', encoding='utf-8')
out_m = io.open('metadata.tsv', 'w', encoding='utf-8')

for index, word in enumerate(vocab):
  if index == 0:
    continue  # skip 0, it's padding.
  vec = weights[index]
  out_v.write('\t'.join([str(x) for x in vec]) + "\n")
  out_m.write(word + "\n")
out_v.close()
out_m.close()

# Download these files if you are running in Colaboratory
try:
  from google.colab import files
  files.download('vectors.tsv')
  files.download('metadata.tsv')
except Exception:
  pass

## Visualize the embeddings
#  To visualize the embeddings, upload two .tsv files to 
#    [Embedding Projector](http://projector.tensorflow.org):
#      (1) vecs.tsv, a file of vectors containing the embedding,
#      (2) meta.tsv, a file of meta data containing the words.

#  #1. Open the [Embedding Projector](http://projector.tensorflow.org/)
#      This can also run in a local TensorBoard instance.
#  #2. Click on "Load data".
#  #3. Upload the two files: `vecs.tsv` and `meta.tsv`.

#  You can search for words to find their closest neighbors. e.g.
#    search for "beautiful". You may see neighbors like "wonderful". 

# Note: Experimentally, you may be able to produce more interpretable embeddings by using a simpler model.
#       Try deleting the `Dense(16)` layer, retraining the model, and visualizing the embeddings again.
# Note: Typically, a much larger dataset is needed to train more interpretable word embeddings.
#       This tutorial uses a small IMDb dataset for the purpose of demonstration.

## Next Steps
#  * [Word2Vec](https://www.tensorflow.org/tutorials/text/word2vec)
#  * [Transformer model for language understanding](https://www.tensorflow.org/text/tutorials/transformer)
